# Databricks notebook source
display(dbutils.fs.ls("dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data"))

# COMMAND ----------

airlines = spark.read.option("header", "true").parquet(f"dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data/201*.parquet")

# COMMAND ----------

# Filter to datset with entries where diverted != 1, cancelled != 1, dep_delay != Null, and arr_delay != Null
airlines = airlines.where('DIVERTED != 1') \
                   .where('CANCELLED != 1') \
                   .filter(airlines['DEP_DEL15'].isNotNull()) \
                   .filter(airlines['ARR_DEL15'].isNotNull())

print(airlines.count())

# COMMAND ----------

def SplitDataset(model_name):
  # Split airlines data into train, dev, test
  test = airlines.where('Year = 2019') # held out
  train, val = airlines.where('Year != 2019').randomSplit([7.0, 1.0], 6)

  # Select a mini subset for the training dataset (~2000 records)
  mini_train = train.sample(fraction=0.0001, seed=6)

  print("train_" + model_name + " size = " + str(train.count()))
  print("mini_train_" + model_name + " size = " + str(mini_train.count()))
  print("val_" + model_name + " size = " + str(val.count()))
  print("test_" + model_name + " size = " + str(test.count()))
  
  return (mini_train, train, val, test) 

mini_train, train, val, test = SplitDataset("")

# COMMAND ----------

display(train.groupBy('DEP_DEL15').count())

# COMMAND ----------

# Variable Transformations


# COMMAND ----------

# Prep Datasets for all 4 Models
mini_train_lr, train_lr, val_lr, test_lr = SplitDataset("lr") # For Shobha
mini_train_nb, train_nb, val_nb, test_nb = SplitDataset("nb") # For Navya
mini_train_dt, train_dt, val_dt, test_dt = SplitDataset("dt") # For Diana
mini_train_svm, train_svm, val_svm, test_svm = SplitDataset("svm") # For Shaji

# COMMAND ----------

# MAGIC %md
# MAGIC ### Task 3 & General Model Exploration
# MAGIC 
# MAGIC #### Models to Explore
# MAGIC - Logistic Regression - Shobha
# MAGIC - Naive Bayes - Navya
# MAGIC - Decision Trees - Diana
# MAGIC - Support Vector Machines - Shaji
# MAGIC 
# MAGIC Based on analysis of these models, we'll want to determine what is likely the best model given mini training dataset (or even what is the best ensemble of models for our task).
# MAGIC 
# MAGIC #### Tasks for each of us (with our given model)
# MAGIC - You can work in this notebook in a separate cell, or construct a separate notebook--whatever works
# MAGIC - There's a copy of the dataset splits prepped in the cell above for each model (e.g. train_lr for training set of Logistic Regression)
# MAGIC    - This way, if we do any variable transformations to a dataset for a given model, this will not affect others models
# MAGIC - Do appropriate data transformations -- share with others as we're doing it in slack
# MAGIC - Use `Dep_Del15` are the outcome variables (as indicator variables) (if curious & have time, do `Arr_Del15`)
# MAGIC - Use variables discussed from last week's deliverable, along with transformation discussed (if they make sense for the model)
# MAGIC     - Departure Delay Model Spec: https://dbc-b1c912e7-d804.cloud.databricks.com/?o=7564214546094626#notebook/610163713970940/command/4370588189239412 
# MAGIC     - Arrival Delay Model Spec: https://dbc-b1c912e7-d804.cloud.databricks.com/?o=7564214546094626#notebook/610163713970940/command/4370588189239413
# MAGIC - Train on mini training set using the models, validate on (mini) validation set
# MAGIC - See about using algorithm via MLLib, own implementation in spark, etc
# MAGIC - Generating metrics for precision and recall (on mini_train & validation set)
# MAGIC - If mini_train is looking good, go to the full train dataset
# MAGIC - Play around with using different variables, fine tuning, transforming the variables, doing variable selection (e.g. with regularization / PCA?)

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ## Decision Trees: Diana
# MAGIC 
# MAGIC #### Things to try
# MAGIC - Transforming variables as decribed before
# MAGIC - Saving data in avro format instead of parquet (since this is a row-wise storage)
# MAGIC - Using data frames & RDDs?
# MAGIC 
# MAGIC Following example: https://databricks.com/blog/2019/05/02/detecting-financial-fraud-at-scale-with-decision-trees-and-mlflow-on-databricks.html

# COMMAND ----------

sc = spark.sparkContext

# COMMAND ----------

# Write data to avro
mini_train_dt.write.format("com.databricks.spark.avro").save("/dbfs/user/team20/airlines_mini_train.avro")
train_dt.write.format("com.databricks.spark.avro").save("/dbfs/user/team20/airlines_train.avro")
val_dt.write.format("com.databricks.spark.avro").save("/dbfs/user/team20/airlines_val.avro")
test_dt.write.format("com.databricks.spark.avro").save("/dbfs/user/team20/airlines_test.avro")

# COMMAND ----------

# Re-read from avro file
mini_train_dt = spark.read.format("com.databricks.spark.avro").load("/dbfs/user/team20/airlines_mini_train.avro")
val_dt = spark.read.format("com.databricks.spark.avro").load("/dbfs/user/team20/airlines_val.avro")

# COMMAND ----------

# Convert to rdd (for now, might not use)
mini_train_rdd = mini_train_dt.rdd
val_rdd = val_dt.rdd

# COMMAND ----------

# Extract only columns of interest
def ExtractColumnsForDepDelayModel(df):
  return df.select('Dep_Del15', 'Year', 'Month', 'Day_Of_Month', 'Day_Of_Week', 'Op_Unique_Carrier', 'Origin', 'Dest', 'CRS_Dep_Time', 'CRS_Arr_Time', 'CRS_Elapsed_Time', 'Distance', 'Distance_Group')

mini_train_dep = ExtractColumnsForDepDelayModel(mini_train_dt)
val_dep = ExtractColumnsForDepDelayModel(val_dt)

# COMMAND ----------

# Register table so it is accessible via SQL Context
mini_train_dep.createOrReplaceTempView("mini_train_dep")

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Looks like we can run SQL directly for EDA -- want to return back to this
# MAGIC select Dep_Del15 from mini_train_dep

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(distinct Dest) from mini_train_dep

# COMMAND ----------

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import DecisionTreeClassifier

# Encodes a string column of labels to a column of label indicies for non-numerical values
# https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe
indexers = [StringIndexer(inputCol=column, outputCol=column+"_idx") for column in ['Op_Unique_Carrier', 'Origin', 'Dest']]

# VectorAssembler is a transformer that combines a given list of columns into a single vector column
# Take all features and merge in
# TODO: Do variable Transformations and discussed previously
va = VectorAssembler(inputCols = ['Year', 'Month', 'Day_Of_Month', 'Day_Of_Week', 
                                  #'Op_Unique_Carrier_idx', 'Origin_idx', 'Dest_idx', 
                                  #'CRS_Dep_Time', 'CRS_Arr_Time', 'CRS_Elapsed_Time', 'Distance', 
                                  'Distance_Group'], 
                     outputCol = "features")

# Use the Decision Tree Classifier Model
# Set max bin to be greater than the number of distinct values for a given feature
# Also commented out more fine-grained feature for now
dt = DecisionTreeClassifier(labelCol = "Dep_Del15", featuresCol = "features", seed = 6, maxDepth = 5, maxBins=32)

# Create our pipeline stages
pipeline = Pipeline(stages=indexers + [va, dt])

# View the Decision Tree Model
dt_model = pipeline.fit(mini_train_dep)

# COMMAND ----------

# Visualize the decision tree model that was trained
display(dt_model.stages[-1])

# COMMAND ----------

# Do Model eval with cross validator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator

paramGrid = ParamGridBuilder() \
  .addGrid(dt.maxDepth, [5, 10, 15]) \
  .addGrid(dt.maxBins, [30, 40, 50]) \
  .build()
crossval = CrossValidator(estimator = dt,
                          estimatorParamMaps = paramGrid,
                          evaluator = BinaryClassificationEvaluator,
                          numFolds = 3)  
pipelineCV = Pipeline(stages=indexers + [va, crossval])

# Train the model using the pipeline and preceding BinaryClassificationEvaluator
cvModel_u = pipelineCV.fit(mini_train_dep)

# COMMAND ----------

# MAGIC %md
# MAGIC # Naive Bayes: Navya

# COMMAND ----------

from pyspark.ml.classification import NaiveBayes
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline

# Use VectorAssembler() to merge our feature columns into a single vector column, which will be passed into the Naive Bayes model. 
# We will not transform the dataset just yet as we will be passing the VectorAssembler into our ML Pipeline.
assembler = VectorAssembler(inputCols = ['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK','CRS_DEP_TIME', 'CRS_ARR_TIME', 'CRS_ELAPSED_TIME', 'DISTANCE', 'DISTANCE_GROUP'], outputCol = "features")

# Train a NaiveBayes model
nb = NaiveBayes(labelCol = "DEP_DEL15", featuresCol = "features", smoothing = 1)

# Chain assembler and nb_model in a pipeline
pipeline = Pipeline(stages=[assembler, nb])

# Run stages in pipeline and train the model
nb_model = pipeline.fit(mini_train)

# Make predictions on test data to measure the accuracy of the model on new data
predictions = nb_model.transform(val)

# Select results to view
display(predictions.select("DEP_DEL15", "prediction", "probability"))


# COMMAND ----------

# Model Evaluation

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

print("Model Evaluation")
print("----------------")

# Accuracy
evaluator = MulticlassClassificationEvaluator(labelCol="DEP_DEL15", predictionCol="prediction",
                                              metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Accuracy: ", accuracy)

# Recall
evaluator = MulticlassClassificationEvaluator(labelCol="DEP_DEL15", predictionCol="prediction",
                                              metricName="weightedRecall")
recall = evaluator.evaluate(predictions)
print("Recall: ", recall)

# Precision
evaluator = MulticlassClassificationEvaluator(labelCol="DEP_DEL15", predictionCol="prediction",
                                              metricName="weightedPrecision")
precision = evaluator.evaluate(predictions)
print("Precision: ", precision)

# F1
evaluator = MulticlassClassificationEvaluator(labelCol="DEP_DEL15", predictionCol="prediction",
                                              metricName="f1")
f1 = evaluator.evaluate(predictions)
print("F1: ", f1)


# COMMAND ----------

# Confusion Matrix

from pyspark.mllib.evaluation import MulticlassMetrics

# Create (prediction, label) pairs and generate confusion matrix
predictionAndLabel = predictions.select("prediction", "DEP_DEL15").rdd
metrics = MulticlassMetrics(predictionAndLabel)
print(metrics.confusionMatrix())

# COMMAND ----------

# Experimenting with Various Smoothing Parameters

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# We can experiment with various smoothing parameters to see which returns the best resultby using ParamGridBuilder and CrossValidator.
# 6 values are used for the smoothing parameter, this grid will provide 6 parameter settings for CrossValidator to model, evaluate and choose from.


# Create ParamGrid and Evaluator for Cross Validation
paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build()
cvEvaluator = MulticlassClassificationEvaluator(metricName="accuracy")

# Run Cross-validation
cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)
cvModel = cv.fit(mini_train)

# # Make predictions on testData. cvModel uses the bestModel.
# cvPredictions = cvModel.transform(val)

# # Select results to view
# display(cvPredictions.select("DEP_DEL15", "prediction", "probability"))

# # Evaluate bestModel found from Cross Validation
# evaluator.evaluate(cvPredictions)

# Does smoothing have any effect on this dataset?

# COMMAND ----------

display(mini_train.select("DAY_OF_WEEK", "DEP_DEL15"))